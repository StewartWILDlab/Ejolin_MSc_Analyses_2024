---
title: "Seasonality"
output: html_document
date: "2023-09-12"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#README

This script is created to define the summer/winter seasons within TDN using the ...image_report.csv, the "image_snow" field, filtered by "Time Lapse" images.The image_snow field was checked Y if the timelapse image had greater than or equal to 50% ground cover of snow, N if the image had less than 50% ground cover of snow. Snow tagging was done by S.Daher and verified by E.Jolin. 

Start and end of the winter season will be delineated by the first and last seven consecutive days of snow cover respectively. Adapted from US National Parks Service protocols described at the North American Caribou Workshop and Arctic Ungulate Conference 2023 by Matthew Cameron. 

These dates need to be calculated at each camera station and the average date taken across stations for the entire study area.

These snow-based seasons are useful but we ended up going with biologically relevant seasons for caribou in TDN instead based on detection data. The code to create these seasons is in 9c_GLMM_Create_Datasets.Rmd  

#Load packages
```{r}
project<-"TDN"
version<-"v14"

# Check you have them and load them
list.of.packages <- c("kableExtra", "tidyr", "leaflet", "dplyr", "viridis", "corrplot", "lubridate", "plotly","stringr","zoo","slider")

new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
lapply(list.of.packages, require, character.only = TRUE)
```


#Load data
```{r}
#load ...image_report.csv generated by WildTrax
image_data_raw<-read.csv("Raw_Data/TDN_RawData_16102023/NWTBM_Thaidene_Nëné_Biodiversity_Project_2021_image_report.csv")
length(unique(image_data_raw$location))

#filter outlier photos from the raw data downloads before going on to data exploration/analysis
#BMS-CRU-004-01 had one single photo taken on 2021-03-03 11:24:10 before being deployed in August 2021
image_data_tmp<-filter(image_data_raw,!((image_id == 60289868) & #filter image by image_id 
                      (image_date_time == " 2021-03-03 11:24:10"))) #and image_date_time
stopifnot(nrow(image_data_tmp)==(nrow(image_data_raw)-1)) #stop if the new df isn't smaller by one row

#BIO-TDN-029-02 wasn't retrieved until 2023 but staff walked passed it on 2022-08-19 13:30:10
image_data<-filter(image_data_tmp,!((location == "BIO-TDN-029-02") & #filter by location
                  (as_datetime(image_date_time)>as_datetime("2022-08-19 13:30:10")))) #filter images taken after 2022-08-19 13:30:10
stopifnot(nrow(image_data)==(nrow(image_data_tmp)-1064)) #stop if the new df isn't smaller by 1064

rm(image_data_tmp) #remove image_data_tmp to clean environment
```

#Create working dataframe
```{r}
#filter data to timelapse images only, with only necessary columns
tl_dat<-subset(image_data,image_trigger_mode == "Time Lapse", select = c("location","image_id","image_date_time","image_snow","image_exif_temperature"))

#replace blank image_snow values with "false"
tl_dat$image_snow<-sub("^$","false",tl_dat$image_snow)

#Select only date and create new coloumn
tl_dat$image_date<-substr(tl_dat$image_date_time,1,11)

#remove original image_date_time clumn
tl_dat<-tl_dat[c('location','image_date','image_snow','image_exif_temperature')]
```

#Find start/end dates for each location and find average start/end end date across all sites
```{r}
tl_dat_arranged<-arrange(tl_dat,location,image_date)

tail(slide(tl_dat_arranged$image_snow, ~.x, .after=6))#check to see if it's tapering off correctly

slide(tl_dat_arranged$image_snow, ~.x=="true", .after=6)

table(tl_dat_arranged$image_snow)#check for extra image_snow values

slide(tl_dat_arranged$image_snow, ~(all(.x=="true")), .after=6)

head(slide(tl_dat_arranged$image_snow, ~(all(.x=="true")), .after=6),100)

winter_table<-tl_dat %>% 
  mutate(image_date=as.Date(image_date))%>% 
  arrange(location,image_date) %>% 
  group_by(location) %>% 
  mutate(winter_start_logical=slide(image_snow, ~(all(.x=="true")), .after=6),
         winter_end_logical=slide(image_snow,~(all(.x=="false")), .after=6)
) %>% 
  mutate(winter_start_date=image_date[first(which(winter_start_logical==TRUE))]) %>% 
  filter(image_date>"2022-01-01") %>% 
  mutate(winter_end_date=rev(image_date)[first(which(rev(winter_start_logical)==TRUE))]) %>% 
  select(location,winter_start_date,winter_end_date)%>% 
  distinct()

any(table(winter_table$location)>1) #check for duplicate stations

#find mean start/end date
winter_table_noNA<-winter_table %>% drop_na() #remove NAs which muck up mean function

mean(winter_table_noNA$winter_start_date) 
mean(winter_table_noNA$winter_end_date)

```
mean_start_date 2021-10-28
mean_end_date 2022-05-11

#Duration of winter at each camera

```{r}
# Calculate the winter_duration in days
winter_table_noNA$winter_duration <- as.numeric(difftime(winter_table_noNA$winter_end_date, winter_table_noNA$winter_start_date, units = "days"))

#tmp_winter<-select(winter_table_noNA,c(location,winter_duration))
```


#Mean winter exif temp

This seems like it would just be correlated with seasonality

Avg monthly temp probably more useful

```{r}

```


#Add to covariate data

Do we need this for our covariate data really?
```{r}
#cov_dat<- read.csv("Processed_Data/TDN_camera_locations_and_covariates_v13.csv")

#locs<-left_join(cov_dat,tmp_winter,by="location")

#write.csv(locs, paste0(project, "_camera_locations_and_covariates_", version, ".csv" ), row.names = F)
```

